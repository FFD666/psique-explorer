# Importamos as bibliotecas necess√°rias
import streamlit as st
import google.generativeai as genai

# --- CONFIGURA√á√ÉO DA IA ---
# Esta parte √© crucial. O Streamlit vai ler a chave do "cofre" (Secrets)
# que configuramos na plataforma Streamlit Community Cloud.
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=GOOGLE_API_KEY)
    modelo = genai.GenerativeModel('gemini-1.5-flash')
except (FileNotFoundError, KeyError):
    # Este bloco √© um fallback para quando rodamos o app localmente
    # e n√£o temos um arquivo de secrets. Ele procura a chave em outro lugar.
    # Se voc√™ for testar localmente, precisar√° configurar isso.
    # Por enquanto, focaremos no funcionamento online.
    st.error("Chave de API do Google n√£o encontrada. Configure-a nos 'Secrets' do Streamlit Cloud.")
    st.stop()


# --- INTERFACE DO APLICATIVO ---
st.title("üß† Psique Explorer v3.0")
st.caption("Seu tutor de psicologia com IA, agora com conversas abertas.")

# Permite ao usu√°rio escolher sua "persona"
persona_selecionada = st.radio(
    "Como voc√™ gostaria de interagir com a IA?",
    ('Estou aqui para Aprender (Estudante)', 'Estou aqui para Debater (Professor)'),
    horizontal=True
)

# Define o comportamento inicial da IA com base na persona
if persona_selecionada == 'Estou aqui para Aprender (Estudante)':
    prompt_inicial = {
        "role": "model",
        "parts": ["Aja como um tutor de psicologia chamado 'Psique Explorer'. Voc√™ √© paciente, did√°tico e adora usar analogias e exemplos pr√°ticos. Seu objetivo √© ajudar estudantes de gradua√ß√£o a entenderem conceitos complexos de forma simples. Comece se apresentando e perguntando qual o t√≥pico de hoje."]
    }
else: # Professor
    prompt_inicial = {
        "role": "model",
        "parts": ["Aja como um colega acad√™mico especialista em psicologia. Voc√™ √© preciso, t√©cnico e capaz de debater nuances te√≥ricas, comparar autores e sugerir materiais de pesquisa. Assuma que o usu√°rio tem conhecimento pr√©vio. Comece se apresentando de forma profissional e se colocando √† disposi√ß√£o para o debate."]
    }

# --- GERENCIAMENTO DA MEM√ìRIA DA CONVERSA ---
# Usamos o 'st.session_state' para que o app n√£o esque√ßa o hist√≥rico do chat
if "historico_chat" not in st.session_state or st.session_state.get('persona_anterior') != persona_selecionada:
    st.session_state.historico_chat = [prompt_inicial]
    st.session_state.persona_anterior = persona_selecionada

# Inicia o modelo de chat com o hist√≥rico guardado
chat = modelo.start_chat(history=[m for m in st.session_state.historico_chat if m['role'] != 'model' or len(st.session_state.historico_chat) == 1])


# --- EXIBI√á√ÉO DO CHAT ---
# Mostra as mensagens antigas na tela
for mensagem in st.session_state.historico_chat:
    with st.chat_message(mensagem["role"]):
        st.markdown(mensagem["parts"][0])

# Pede por uma nova mensagem do usu√°rio
if prompt_usuario := st.chat_input("Digite sua mensagem..."):
    # Adiciona a mensagem do usu√°rio ao hist√≥rico e exibe na tela
    st.session_state.historico_chat.append({"role": "user", "parts": [prompt_usuario]})
    with st.chat_message("user"):
        st.markdown(prompt_usuario)

    # Envia a conversa para a IA e espera a resposta
    with st.chat_message("model"):
        message_placeholder = st.empty()
        resposta_completa = ""
        try:
            # Envia a nova mensagem para o chat continuar a conversa
            resposta = chat.send_message(prompt_usuario, stream=True)
            for chunk in resposta:
                resposta_completa += chunk.text
                message_placeholder.markdown(resposta_completa + "‚ñå")
            message_placeholder.markdown(resposta_completa)
        except Exception as e:
            st.error(f"Ocorreu um erro ao processar sua mensagem. Verifique sua chave de API ou conex√£o. Detalhes: {e}")